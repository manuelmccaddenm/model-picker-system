{
  "theory": [
    {
      "model_id": "LinearRegression",
      "task": "regression",
      "family": "Linear",
      "description": "Standard linear model that assumes a linear relationship between features and the target. Minimizes mean squared error (MSE) without regularization.",
      "strengths": [
        "Maximum interpretability (directly interpretable coefficients).",
        "Extremely fast to train and predict (closed-form analytical solution).",
        "No complex hyperparameters to tune.",
        "Robust baseline and standard starting point.",
        "Provides confidence intervals and statistical metrics (RÂ², p-values)."
      ],
      "limitations": [
        "Cannot capture non-linear relationships without manual feature transformation.",
        "Very sensitive to outliers that can distort the model.",
        "Assumes homoscedasticity (constant variance of errors).",
        "Suffers from multicollinearity when features are correlated (unstable coefficients).",
        "Does not perform automatic feature selection.",
        "Can overfit with many features and little data."
      ],
      "recommendations_use_when": [
        "Interpretability is the top priority.",
        "A fast baseline is needed for comparison.",
        "The relationship between variables appears approximately linear.",
        "Explaining the impact of each feature is required (statistical inference).",
        "The dataset is small to medium and features are not highly correlated."
      ],
      "recommendations_avoid_when": [
        "The problem is clearly non-linear.",
        "Predictive accuracy is the top priority.",
        "There are many correlated features (consider regularization).",
        "Automatic feature selection is required."
      ],
      "tradeoff_scores": {
        "interpretability": 1.0,
        "predictive_accuracy": 0.3,
        "computational_cost": 1.0,
        "outlier_robustness": 0.2
      },
      "tags": [
        "linear",
        "interpretable",
        "baseline",
        "parametric"
      ]
    },
    {
      "model_id": "RandomForestRegressor",
      "task": "regression",
      "family": "Ensemble (Bagging)",
      "description": "Ensemble of decision trees trained on random subsets of data and features. Averages predictions to reduce variance.",
      "strengths": [
        "High predictive accuracy in most problems.",
        "Very robust to overfitting (thanks to bagging).",
        "Handles non-linear relationships and complex interactions well.",
        "Does not require feature scaling.",
        "Handles missing values reasonably well.",
        "Provides useful 'feature importance'.",
        "Robust to outliers compared to linear models.",
        "Easy to use with good default parameters."
      ],
      "limitations": [
        "Low interpretability (black box model).",
        "Longer training time than linear models.",
        "Longer prediction time than simple models.",
        "Higher memory usage than a single tree.",
        "Cannot extrapolate beyond training data range.",
        "Can be biased towards features with many categories or unique values.",
        "Less accurate than more advanced Gradient Boosting models in many cases."
      ],
      "recommendations_use_when": [
        "Accuracy is more important than interpretability.",
        "Non-linear complex relationships are suspected.",
        "It's the first complex model to try (all-purpose).",
        "Robustness to overfitting is needed without much tuning.",
        "There are missing values in the data.",
        "Feature importance is desired."
      ],
      "recommendations_avoid_when": [
        "Interpretability is the top priority.",
        "Training or prediction time is extremely limited.",
        "Memory is very limited.",
        "Extrapolation beyond training data is required."
      ],
      "tradeoff_scores": {
        "interpretability": 0.3,
        "predictive_accuracy": 0.8,
        "computational_cost": 0.5,
        "outlier_robustness": 0.7
      },
      "tags": [
        "ensemble",
        "bagging",
        "non-linear",
        "robust",
        "feature-importance"
      ]
    },
    {
      "model_id": "MLPRegressor",
      "task": "regression",
      "family": "Neural Network",
      "description": "Multi-Layer Perceptron. Feedforward neural network with hidden layers that can learn arbitrarily complex non-linear functions.",
      "strengths": [
        "Potential to model extremely complex and non-linear relationships.",
        "Can learn hierarchical feature representations.",
        "Capable of approximating any function (universal approximation theorem).",
        "Flexible architecture (number of layers and neurons).",
        "Can process high-dimensional features."
      ],
      "limitations": [
        "Almost zero interpretability (complete black box).",
        "REQUIRES feature scaling/normalization (very sensitive).",
        "Many hyperparameters (architecture, learning_rate, activation, optimizer).",
        "Slow and computationally expensive training.",
        "Prone to getting stuck in local minima.",
        "Requires large amounts of data for good performance.",
        "Very difficult to debug and tune.",
        "Limited sklearn implementation (consider PyTorch/TensorFlow for complex cases)."
      ],
      "recommendations_use_when": [
        "The problem is extremely complex and non-linear.",
        "You have a large amount of data (> 10k observations).",
        "Accuracy is the only absolute priority.",
        "Everything else has been tried without success.",
        "You have deep learning experience.",
        "Significant computational resources are available."
      ],
      "recommendations_avoid_when": [
        "Interpretability is important.",
        "The dataset is small (< 10k observations).",
        "Features are not scaled.",
        "A quick result is needed.",
        "You lack experience with neural networks.",
        "Simpler models have not been tried first."
      ],
      "tradeoff_scores": {
        "interpretability": 0.05,
        "predictive_accuracy": 0.85,
        "computational_cost": 0.2,
        "outlier_robustness": 0.3
      },
      "tags": [
        "neural-network",
        "deep-learning",
        "non-linear",
        "complex",
        "scaling-required",
        "black-box"
      ]
    },
    {
      "model_id": "ARIMA",
      "task": "regression",
      "family": "Time Series",
      "description": "Autoregressive Integrated Moving Average. Classic statistical model for univariate time series that combines auto-regression, integration (differencing), and moving averages.",
      "strengths": [
        "Solid and well-established statistical theoretical foundation.",
        "Excellent for capturing trends and auto-correlation patterns.",
        "Interpretable through p, d, q parameters.",
        "Can be extended to SARIMA for seasonality.",
        "Works well with short time series.",
        "Provides confidence intervals for forecasts.",
        "Standard model in econometrics and finance."
      ],
      "limitations": [
        "ONLY works with univariate time series data.",
        "Requires stationarity (needs differencing, parameter 'd').",
        "Complex to tune (finding optimal p, d, q).",
        "Does not easily handle external regressors (additional features).",
        "Assumes linearity in temporal relationships.",
        "Can be outperformed by more modern models on complex series.",
        "Difficult with series having multiple complex seasonalities."
      ],
      "recommendations_use_when": [
        "The problem is a univariate time series.",
        "Clear trends or auto-correlation patterns exist.",
        "A robust traditional statistical model is required.",
        "Statistically valid confidence intervals are needed.",
        "The forecast horizon is short to medium term.",
        "Working in economics or finance (standard model)."
      ],
      "recommendations_avoid_when": [
        "The problem is not a time series.",
        "There are many important external/exogenous features.",
        "The series has multiple complex seasonalities.",
        "External events or holidays need to be easily incorporated."
      ],
      "tradeoff_scores": {
        "interpretability": 0.7,
        "predictive_accuracy": 0.65,
        "computational_cost": 0.6,
        "outlier_robustness": 0.4
      },
      "tags": [
        "time-series",
        "univariate",
        "statistical",
        "stationarity",
        "auto-correlation"
      ]
    },
    {
      "model_id": "Prophet",
      "task": "regression",
      "family": "Time Series",
      "description": "Time series model from Meta (Facebook) designed to be robust to missing data, outliers, and multiple seasonalities. Uses a decomposable additive model.",
      "strengths": [
        "Excellent handling of multiple seasonalities (daily, weekly, annual).",
        "Very robust to missing data and outliers.",
        "Handles external regressors (holidays, events) natively.",
        "Fast and easy to use with good default parameters.",
        "Does not require the series to be stationary.",
        "Provides interpretable components (trend, seasonality).",
        "Excellent for business forecasting.",
        "Automatic component visualizations."
      ],
      "limitations": [
        "ONLY works with time series data.",
        "Can be outperformed by fine-tuned statistical models on simple series.",
        "Less flexible than deep learning models for very complex patterns.",
        "Assumes additive or multiplicative seasonalities (not both simultaneously).",
        "Not as good with very short series (< 2 seasonal cycles).",
        "Can overfit with too many regressors."
      ],
      "recommendations_use_when": [
        "The time series has clear seasonal patterns (multiple).",
        "Important holidays or external events exist.",
        "There is missing data or outliers in the series.",
        "A robust and fast model is needed without much tuning.",
        "Interpretable decomposition (trend + seasonality) is required.",
        "Working with business data (sales, users, metrics)."
      ],
      "recommendations_avoid_when": [
        "The problem is not a time series.",
        "The series is very short (< 2 complete cycles).",
        "Maximum accuracy is required at any cost.",
        "The series has no clear seasonal patterns."
      ],
      "tradeoff_scores": {
        "interpretability": 0.65,
        "predictive_accuracy": 0.75,
        "computational_cost": 0.7,
        "outlier_robustness": 0.8
      },
      "tags": [
        "time-series",
        "seasonality",
        "external-regressors",
        "robust",
        "holidays"
      ]
    },
    {
      "model_id": "LogisticRegression",
      "task": "classification",
      "family": "Linear",
      "description": "Linear model for classification that models the probability of class membership using the logistic (sigmoid) function.",
      "strengths": [
        "Maximum interpretability (coefficients and odds ratios).",
        "Extremely fast to train and predict.",
        "Provides naturally calibrated probabilities.",
        "Robust baseline for binary and multiclass classification.",
        "Works well with features linearly related to log-odds.",
        "L1/L2 regularization available in sklearn.",
        "Extensible to multiclass (one-vs-rest, multinomial)."
      ],
      "limitations": [
        "Cannot capture non-linear relationships (linear decision boundary).",
        "Assumes independence of observations.",
        "Can suffer from perfect or quasi-perfect separation.",
        "Sensitive to multicollinearity without regularization.",
        "Limited performance on complex non-linear problems."
      ],
      "recommendations_use_when": [
        "Interpretability is the top priority.",
        "A fast baseline is needed.",
        "Well-calibrated probabilities are required.",
        "The relationship between features and log-odds is approximately linear.",
        "Explaining the impact of each feature is necessary.",
        "Simple binary or multiclass classification problems."
      ],
      "recommendations_avoid_when": [
        "The problem is clearly non-linear.",
        "Predictive accuracy is the top priority.",
        "There are complex feature interactions.",
        "Very complex patterns need to be captured."
      ],
      "tradeoff_scores": {
        "interpretability": 1.0,
        "predictive_accuracy": 0.45,
        "computational_cost": 1.0,
        "outlier_robustness": 0.3
      },
      "tags": [
        "linear",
        "interpretable",
        "baseline",
        "probabilities",
        "classification"
      ]
    },
    {
      "model_id": "GaussianNB",
      "task": "classification",
      "family": "Probabilistic",
      "description": "Naive Bayes classifier that assumes conditional independence between features given the class, and Gaussian distribution of continuous features.",
      "strengths": [
        "Extremely fast to train and predict.",
        "Works surprisingly well with high dimensionality (e.g., text).",
        "Requires relatively little training data.",
        "Provides probabilities.",
        "Robust to irrelevant features (thanks to independence assumption).",
        "Simple and easy to implement.",
        "Works well as a fast baseline."
      ],
      "limitations": [
        "The 'naive independence' assumption is almost never true in reality.",
        "Can fail when features are highly correlated.",
        "Assumes Gaussian (Normal) distribution for continuous features.",
        "Probabilities can be poorly calibrated.",
        "Does not capture feature interactions.",
        "Limited performance compared to more sophisticated models."
      ],
      "recommendations_use_when": [
        "Speed is critical.",
        "The dataset is very high-dimensional (e.g., NLP, bag-of-words).",
        "An extremely fast baseline is needed.",
        "The training dataset is small.",
        "Features are truly independent or nearly independent."
      ],
      "recommendations_avoid_when": [
        "Features are clearly correlated.",
        "Maximum accuracy is needed.",
        "Feature interactions are important.",
        "Features do not follow Gaussian distribution."
      ],
      "tradeoff_scores": {
        "interpretability": 0.7,
        "predictive_accuracy": 0.5,
        "computational_cost": 1.0,
        "outlier_robustness": 0.5
      },
      "tags": [
        "probabilistic",
        "naive-bayes",
        "baseline",
        "fast",
        "text",
        "high-dimension"
      ]
    },
    {
      "model_id": "SVC",
      "task": "classification",
      "family": "Kernel (SVM)",
      "description": "Support Vector Classifier. Finds the optimal hyperplane that maximizes the margin between classes. Can use kernels for non-linear problems.",
      "strengths": [
        "Very high accuracy, especially in high-dimensional spaces.",
        "Effective when number of features > number of samples.",
        "Flexible through different kernels (linear, rbf, poly, sigmoid).",
        "Robust to overfitting in high-dimensional spaces.",
        "Memory efficient (only uses support vectors).",
        "Works well with complex decision boundaries (rbf kernel).",
        "If dataset has more than 25 features, SVC is a good choice."
      ],
      "limitations": [
        "Very slow to train on large datasets (O(nÂ²) to O(nÂ³) complexity).",
        "Does not provide probabilities directly (requires calibration).",
        "VERY sensitive to feature scaling (REQUIRES standardization).",
        "Difficult to interpret, especially with non-linear kernels.",
        "Many hyperparameters to tune (C, gamma, kernel).",
        "Poor with datasets having much noise or class overlap.",
        "Does not handle missing values natively."
      ],
      "recommendations_use_when": [
        "Maximum accuracy is the priority.",
        "The dataset is high-dimensional (e.g., images, text).",
        "Number of features > number of samples.",
        "The problem is non-linear (using 'rbf' kernel).",
        "The dataset is small to medium (< 10k observations).",
        "Classes are well separated."
      ],
      "recommendations_avoid_when": [
        "The dataset is very large (> 10k observations).",
        "Interpretability is important.",
        "Training speed is critical.",
        "Features are not scaled.",
        "Well-calibrated probabilities are needed directly."
      ],
      "tradeoff_scores": {
        "interpretability": 0.2,
        "predictive_accuracy": 0.88,
        "computational_cost": 0.25,
        "outlier_robustness": 0.4
      },
      "tags": [
        "kernel",
        "svm",
        "non-linear",
        "high-accuracy",
        "scaling-required",
        "high-dimension"
      ]
    },
    {
      "model_id": "RandomForestClassifier",
      "task": "classification",
      "family": "Ensemble (Bagging)",
      "description": "Ensemble of decision trees for classification. Each tree votes for a class and the final prediction is the most voted class.",
      "strengths": [
        "High predictive accuracy in most problems.",
        "Very robust to overfitting.",
        "Handles non-linear relationships and interactions well.",
        "Provides 'feature importance' and probabilities.",
        "Does not require feature scaling.",
        "Handles missing values reasonably.",
        "Robust to outliers.",
        "Works well with imbalanced data (using class_weight)."
      ],
      "limitations": [
        "Low interpretability (black box).",
        "Slower than linear models.",
        "Higher memory usage.",
        "Can be biased towards features with many categories.",
        "Probabilities can be poorly calibrated (requires calibration).",
        "Less accurate than more advanced Gradient Boosting models in many cases."
      ],
      "recommendations_use_when": [
        "Accuracy is more important than interpretability.",
        "It's the first complex model to try (all-purpose).",
        "Non-linear relationships are suspected.",
        "Robustness is needed without much tuning.",
        "There are missing values in the data.",
        "Feature importance is desired."
      ],
      "recommendations_avoid_when": [
        "Interpretability is the top priority.",
        "Training time is extremely limited.",
        "Memory is very limited.",
        "Very well-calibrated probabilities are needed."
      ],
      "tradeoff_scores": {
        "interpretability": 0.3,
        "predictive_accuracy": 0.82,
        "computational_cost": 0.5,
        "outlier_robustness": 0.7
      },
      "tags": [
        "ensemble",
        "bagging",
        "non-linear",
        "robust",
        "probabilities",
        "feature-importance"
      ]
    },
    {
      "model_id": "GradientBoostingClassifier",
      "task": "classification",
      "family": "Ensemble (Boosting)",
      "description": "Gradient Boosting for classification from sklearn. Builds trees sequentially to optimize the loss function (typically log-loss).",
      "strengths": [
        "Exceptional accuracy potential on tabular data.",
        "Captures interactions and complex patterns very well.",
        "Provides useful 'feature importance'.",
        "Handles different types of features well.",
        "Provides reasonably calibrated probabilities."
      ],
      "limitations": [
        "Very sensitive to hyperparameters (learning_rate, n_estimators, max_depth).",
        "Prone to overfitting without proper regularization.",
        "Sequential training (not parallelizable across trees).",
        "Slower than Random Forest to train.",
        "Requires much tuning time for good results.",
        "Sklearn implementation slower than modern alternatives (XGBoost/LightGBM)."
      ],
      "recommendations_use_when": [
        "Maximum accuracy is the priority.",
        "Prefer using only sklearn for consistency.",
        "The dataset is not extremely large.",
        "Time can be dedicated to extensive hyperparameter tuning."
      ],
      "recommendations_avoid_when": [
        "Training time is critical (consider faster implementations).",
        "The dataset is very large (> 100k rows).",
        "No time for detailed tuning.",
        "Interpretability is critical."
      ],
      "tradeoff_scores": {
        "interpretability": 0.25,
        "predictive_accuracy": 0.88,
        "computational_cost": 0.35,
        "outlier_robustness": 0.5
      },
      "tags": [
        "ensemble",
        "boosting",
        "high-accuracy",
        "tuning-sensitive",
        "sequential"
      ]
    }
  ],
  "experiences": [
    {
      "task": "multiclass_classification",
      "dataset": "iris.csv",
      "outcome": "LogisticRegression with C=3.0 achieved near-perfect accuracy (â0.998) and the lowest J=0.0012, far outperforming SVM when interpretability and computational simplicity are prioritized.",
      "learnings": "Linear models remain optimal for multiclass tabular problems when interpretability is a main business constraint; tuning beyond this provides negligible additional business value.",
      "id": "exp_192dfa6809eb410e95b9195de1fe2818",
      "timestamp": "2025-10-23T23:57:00.215044+00:00"
    }
  ],
  "schema_version": "1.0",
  "updated_at": "2025-10-23T23:57:00.216866+00:00",
  "analyst_lessons": [
    {
      "lesson": "For small, clean, multiclass classification tasks with a strong interpretability requirement, Logistic Regression provides the best balance of accuracy and transparency.",
      "source": "slow_loop",
      "run_id": "20251023_175424",
      "id": "lesson_e68d97289f0745128936ea32df12de86",
      "last_updated": "2025-10-23T23:57:00.215715+00:00"
    },
    {
      "lesson": "When strong business constraints favor interpretability, SVMs are usually non-competitive relative to linear models, even if their performance is similar.",
      "source": "slow_loop",
      "run_id": "20251023_175424",
      "id": "lesson_87fcea30c89548ae93ac3ba6119c5aa8",
      "last_updated": "2025-10-23T23:57:00.216290+00:00"
    },
    {
      "lesson": "Hyperparameter tuning of strong baseline interpretable models provides diminishing returns when J is already extremely low.",
      "source": "slow_loop",
      "run_id": "20251023_175424",
      "id": "lesson_029da57903bc407dbfd319af24f5cc08",
      "last_updated": "2025-10-23T23:57:00.216852+00:00"
    }
  ]
}